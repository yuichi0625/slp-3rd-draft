{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b9accbad",
   "metadata": {},
   "source": [
    "## 4.1\n",
    "Assume the following likelihoods for each word being part of a positive or negative movie review, and equal prior probabilities for each class.\n",
    "\n",
    "|  | pos | neg |\n",
    "| --- | --- | --- |\n",
    "| I | 0.09 | 0.16 |\n",
    "| always | 0.07 | 0.06 |\n",
    "| like | 0.29 | 0.06 |\n",
    "| foreign | 0.04 | 0.15 |\n",
    "| films | 0.08 | 0.11 |\n",
    "\n",
    "What class will Naive bayes assign to the sentence “I always like foreign films.”?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6f60630f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "negative\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "P(c|d) = P(d|c)P(c) = P(c)Pi(wi|c)\n",
    "\"\"\"\n",
    "import pandas as pd\n",
    "\n",
    "df = pd.DataFrame(\n",
    "    [[0.09, 0.16],\n",
    "     [0.07, 0.06],\n",
    "     [0.29, 0.06],\n",
    "     [0.04, 0.15],\n",
    "     [0.08, 0.11]],\n",
    "    columns = ('pos', 'neg'),\n",
    "    index=('I', 'always', 'like', 'foreign', 'films')\n",
    ")\n",
    "\n",
    "prob_pos = (df.pos.sum() / df.sum().sum()) * df.pos.prod()\n",
    "prob_neg = (df.neg.sum() / df.sum().sum()) * df.neg.prod()\n",
    "\n",
    "print('positive' if prob_pos > prob_neg else 'negative')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a422471",
   "metadata": {},
   "source": [
    "## 4.2\n",
    "Given the following short movie reviews, each labeled with a genre, either comedy or action:\n",
    "1. fun, couple, love, love **comedy**\n",
    "2. fast, furious, shoot **action**\n",
    "3. couple, fly, fast, fun, fun **comedy**\n",
    "4. furious, shoot, shoot, fun **action**\n",
    "5. fly, fast, shoot, love **action**\n",
    "\n",
    "and a new document D:  \n",
    "&emsp;*fast, couple, shoot, fly*  \n",
    "compute the most likely class for D. Assume a naive Bayes classifier and use\n",
    "add-1 smoothing for the likelihoods."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "23e90317",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "from collections import defaultdict\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "\n",
    "class NaiveBayes:\n",
    "    def __init__(self, binary=False):\n",
    "        self.binary = binary\n",
    "        self.trained = False\n",
    "\n",
    "    def __call__(self, doc):\n",
    "        if not self.trained:\n",
    "            return ''\n",
    "\n",
    "        words = self._tokenize(doc)\n",
    "        sums = []\n",
    "        for c in self.classes:\n",
    "            sums.append(self.log_prior[c] + sum([self.log_likelihood[c][word] for word in words]))\n",
    "\n",
    "        return self.classes[sums.index(max(sums))]\n",
    "\n",
    "    def train(self, data, labels):\n",
    "        self.classes = list(set(labels))\n",
    "        num_doc = 0\n",
    "        num_c = {c: 0 for c in self.classes}\n",
    "        self.vocab = set()\n",
    "        big_doc = {c: defaultdict(int) for c in self.classes}\n",
    "\n",
    "        for doc, label in zip(data, labels):\n",
    "            num_doc += 1\n",
    "            num_c[label] += 1\n",
    "\n",
    "            words = set(self._tokenize(doc)) if self.binary else self._tokenize(doc)\n",
    "            for word in words:\n",
    "                self.vocab.add(word)\n",
    "                big_doc[label][word] += 1\n",
    "\n",
    "        self.log_prior = {c: math.log(num / num_doc) for c, num in num_c.items()}\n",
    "\n",
    "        self.log_likelihood = {c: {} for c in self.classes}\n",
    "        for c in self.classes:\n",
    "            sum_den = sum(big_doc[c].values())\n",
    "            for word in self.vocab:\n",
    "                self.log_likelihood[c][word] = math.log((big_doc[c][word] + 1) / (sum_den + len(self.vocab)))\n",
    "\n",
    "        self.trained = True\n",
    "\n",
    "    def _tokenize(self, doc):\n",
    "        puncts = set(',.!?\\'\"')\n",
    "        if self.trained:\n",
    "            words = [word for word in word_tokenize(doc) if word in self.vocab]\n",
    "        else:\n",
    "            words = [word for word in word_tokenize(doc) if word not in puncts]\n",
    "        return words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3e4e3055",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'action'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "docs = ['fun, couple, love, love',\n",
    "        'fast, furious, shoot',\n",
    "        'couple, fly, fast, fun, fun',\n",
    "        'furious, shoot, shoot, fun',\n",
    "        'fly, fast, shoot, love']\n",
    "labels = ['comedy', 'action', 'comedy', 'action', 'action']\n",
    "\n",
    "nb = NaiveBayes()\n",
    "nb.train(docs, labels)\n",
    "\n",
    "nb('fast, couple, shoot, fly')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7ce8205",
   "metadata": {},
   "source": [
    "## 4.3\n",
    "Train two models, multinomial naive Bayes and binarized naive Bayes, both with add-1 smoothing, on the following document counts for key sentiment words, with positive or negative class assigned as noted.\n",
    "\n",
    "| doc | “good” | “poor” | “great” | (class) |\n",
    "| --- | --- | --- | --- | --- |\n",
    "| d1. | 3 | 0 | 3 | pos |\n",
    "| d2. | 0 | 1 | 2 | pos |\n",
    "| d3. | 1 | 3 | 0 | neg |\n",
    "| d4. | 1 | 5 | 2 | neg |\n",
    "| d5. | 0 | 2 | 0 | neg |\n",
    "\n",
    "Use both naive Bayes models to assign a class (pos or neg) to this sentence:  \n",
    "&emsp;*A good, good plot and great characters, but poor acting.*  \n",
    "Recall from page 6 that with naive Bayes text classification, we simply ignore (throw out) any word that never occurred in the training document. (We don’t throw out words that appear in some classes but not others; that’s what addone smoothing is for.) Do the two models agree or disagree?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b27069c0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>model</th>\n",
       "      <th>result</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>multinomial naive Bayes</td>\n",
       "      <td>pos</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>binarized naive Bayes</td>\n",
       "      <td>neg</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                     model result\n",
       "0  multinomial naive Bayes    pos\n",
       "1    binarized naive Bayes    neg"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "docs = [('good ' * 3 + 'poor ' * 0 + 'great ' * 3).strip(),\n",
    "        ('good ' * 0 + 'poor ' * 1 + 'great ' * 2).strip(),\n",
    "        ('good ' * 1 + 'poor ' * 3 + 'great ' * 0).strip(),\n",
    "        ('good ' * 1 + 'poor ' * 5 + 'great ' * 2).strip(),\n",
    "        ('good ' * 0 + 'poor ' * 2 + 'great ' * 0).strip()]\n",
    "labels = ['pos', 'pos', 'neg', 'neg', 'neg']\n",
    "\n",
    "sentence = 'A good, good plot and great characters, but poor acting.'\n",
    "\n",
    "nb1 = NaiveBayes()\n",
    "nb1.train(docs, labels)\n",
    "result1 = nb1(sentence)\n",
    "\n",
    "nb2 = NaiveBayes(binary=True)\n",
    "nb2.train(docs, labels)\n",
    "result2 = nb2(sentence)\n",
    "\n",
    "pd.DataFrame(\n",
    "    [['multinomial naive Bayes', result1], ['binarized naive Bayes', result2]],\n",
    "    columns=('model', 'result')\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21a0543c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
